{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TUAPDataset(Dataset):\n",
    "    \"\"\"Dataset for TUAP.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, sequence_length, train=True, normalize=True):\n",
    "        \"\"\"Initialize the dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        csv_path : str\n",
    "            Path to the csv file.\n",
    "        sequence_length : int\n",
    "            Length of the sequence.\n",
    "        train : bool, optional\n",
    "            Whether to use the train set or the test set, by default True\n",
    "        normalize : bool, optional\n",
    "            Whether to normalize the data, by default True\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.sequence_length = sequence_length\n",
    "        # self.data['hour'] = self.data['timestamp'].apply(lambda x: int(x.split(' ')[1].split(':')[0]))\n",
    "        self.data['date'] = self.data['timestamp'].apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "        # Get all the data for each day\n",
    "        self.data = self.data.groupby('date').apply(lambda x: x.values).values\n",
    "\n",
    "        # Drop datetime column\n",
    "        self.data = np.array([x[:, 1:-1] for x in self.data])\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get the item at the given index.\"\"\"\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/512hb8bd7ld3l5z5s_8d4b7c0000gn/T/ipykernel_6194/1835614381.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.data = np.array([x[:, 1:-1] for x in self.data])\n"
     ]
    }
   ],
   "source": [
    "symbol='AAPL'\n",
    "dataset = TUAPDataset(csv_path=f'dataset/splitted_s&p500/{symbol}.csv', sequence_length=5, train=True, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate([x[0], x[1], x[2]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = len(x)-35\n",
    "idx_arr = np.random.uniform(0,max_idx,120)\n",
    "samples = [x[int(i):int(i)+35] for i in idx_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.array(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([159.03, 159.07, 159.01, 159.03, 32760.0], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32760.0, 20842.0, 165571.0, 40125.0, 110001.0, 32729.0, 20368.0,\n",
       "       125560.0, 166104.0, 430885.0, 184642.0, 202289.0, 289582.0,\n",
       "       20186.0, 83954.0, 150958.0, 123444.0, 70595.0, 155470.0, 26447.0,\n",
       "       208900.0, 186812.0, 54054.0, 175115.0, 24401.0, 197132.0, 64190.0,\n",
       "       256854.0, 138548.0, 57484.0, 138816.0, 126531.0, 18674.0, 49520.0,\n",
       "       104130.0, 89110.0, 39486.0, 25742.0, 62680.0, 41340.0, 117504.0,\n",
       "       343254.0, 49738.0, 199332.0, 93332.0, 24499.0, 39671.0, 78808.0,\n",
       "       59204.0, 333048.0, 139917.0, 202289.0, 94952.0, 53250.0, 27610.0,\n",
       "       42508.0, 105799.0, 179682.0, 37992.0, 184182.0, 42063.0, 117504.0,\n",
       "       63214.0, 38907.0, 66009.0, 85595.0, 52541.0, 219161.0, 35969.0,\n",
       "       32708.0, 135657.0, 405334.0, 285947.0, 77575.0, 48084.0, 190257.0,\n",
       "       135304.0, 65813.0, 45180.0, 149598.0, 255539.0, 41413.0, 390365.0,\n",
       "       352803.0, 135383.0, 59479.0, 72936.0, 48943.0, 32973.0, 45059.0,\n",
       "       42474.0, 85166.0, 99391.0, 236697.0, 68975.0, 71444.0, 84759.0,\n",
       "       63763.0, 117504.0, 54596.0, 201990.0, 154706.0, 16812.0, 63044.0,\n",
       "       41144.0, 83556.0, 133963.0, 139917.0, 160303.0, 104130.0, 126531.0,\n",
       "       42508.0, 452447.0, 44182.0, 136830.0, 322518.0, 90478.0, 58458.0,\n",
       "       205328.0, 20186.0], dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[:, 1, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (35, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(input_shape,dtype=np.float32)\n",
    "for i in range(25):\n",
    "    mask[i+5,3] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros((8,35,7), dtype=np.float32)\n",
    "target = np.zeros((8,2), dtype=np.float32)\n",
    "global_uap = np.zeros(input_shape, dtype=np.float32)\n",
    "perturbation = np.zeros(input_shape, dtype=np.float32)\n",
    "\n",
    "x_adv = data + (global_uap + perturbation)*mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 35, 7)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first batch of data\n",
    "x, y = next(iter(train_loader))\n",
    "\n",
    "# Get the loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fooling_targets = [torch.tensor([0., 1.]), torch.tensor([1., 0.])]\n",
    "\n",
    "current_target = fooling_targets[0]\n",
    "\n",
    "# Create perturbation to the xi\n",
    "delta = torch.zeros_like(x[0], requires_grad=True)\n",
    "\n",
    "for xi in x:\n",
    "    # Add the perturbation to the xi\n",
    "    x_perturbed = xi + delta\n",
    "\n",
    "    # Format data for model\n",
    "    input_perturbed = create_input_data(x_perturbed.detach().numpy())\n",
    "    input_xp = input_perturbed[:, :-2]\n",
    "    input_xp = torch.from_numpy(input_xp).float()\n",
    "\n",
    "    # Get the model's prediction\n",
    "    y_pred_perturbed = model(input_xp)\n",
    "    y_pred_perturbed = torch.sigmoid(y_pred_perturbed)\n",
    "\n",
    "    # Duplicate the current_target to match the shape of y_pred_perturbed\n",
    "    target = current_target.repeat(y_pred_perturbed.shape[0], 1)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = criterion(y_pred_perturbed, target)\n",
    "\n",
    "    # Calculate the gradients for delta\n",
    "    loss.backward()\n",
    "\n",
    "    # Update delta\n",
    "    delta.data = delta.data + 0.01 * delta.grad.data\n",
    "\n",
    "    # Zero the gradients\n",
    "    delta.grad.data.zero_()\n",
    "    \n",
    "    # Check if the model's prediction is the same as the target\n",
    "    if torch.all(torch.eq(y_pred_perturbed, target)):\n",
    "        print(\"Found the perturbation!\")\n",
    "        break\n",
    "\n",
    "print(f\"delta: {delta}\")\n",
    "\n",
    "\n",
    "pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c89de5159f534544b63f016b62ed0e6fd5607e61ef68177ce6d2410074062219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
